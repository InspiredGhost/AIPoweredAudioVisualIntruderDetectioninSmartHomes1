\documentclass[11pt]{article}
\usepackage{times}
\usepackage{enumitem}
\usepackage{mathptmx} % Times New Roman font (Scholar font)
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}
\usepackage{longtable}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{url}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue,
}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage{fontenc}
\geometry{a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm}

\begin{document}

\begin{center}
\vspace{3cm}
{\huge \textbf{-- RESEARCH PROPOSAL --}}

\vspace{2cm}

{\Large \textbf{AI-POWERED AUDIO-VISUAL INTRUDER DETECTION SYSTEM FOR SMART HOMES}}

\vspace{3cm}

\textbf{BY}

\vspace{0.5cm}

\textbf{Desmond Makhubela} \\
\textbf{Student Number: 214396874}

\vspace{2cm}

Submitted in fulfilment of the requirements for the degree \\
\textbf{N. Dip. Computer Systems Engineering} \\
At the Department of Computer Systems Engineering \\
In the \\
\textbf{FACULTY OF INFORMATION AND COMMUNICATION TECHNOLOGY} \\
At the \textbf{TSHWANE UNIVERSITY OF TECHNOLOGY}

\vspace{2cm}

\textbf{Supervisors:} \\
Munguakonkwa Emmanuel Migabo \\
Oluwasogo Moses Olaifa \\
Chunling Du

\vspace{2cm}

\end{center}

\newpage

\section*{Abstract}

This research proposal outlines the development and implementation of an AI-powered audio-visual intruder detection system for smart home environments. Traditional security mechanisms, such as motion sensors and closed-circuit television (CCTV), are plagued by high false alarm rates, leading to reduced user trust and effectiveness. The research problem addresses the persistent limitations in conventional intruder detection systems by proposing to build a practical, cost-effective multimodal AI system that combines audio and visual processing for enhanced threat detection accuracy. The proposed methodology employs a comprehensive system development approach, including hardware platform selection (Raspberry Pi for edge computing), data collection and preprocessing, AI model development and training using convolutional neural networks (CNNs) and recurrent neural networks (RNNs), and multimodal fusion implementation. The system will integrate real-time audio analysis through microphone arrays with visual processing via camera modules, implementing advanced fusion techniques (early, late, and hybrid) to minimize false alarms while maintaining high detection accuracy. Expected outcomes include a functional prototype system achieving >95% detection accuracy with <1 false alarm per day, comprehensive performance evaluation in real-world scenarios, and deployment guidelines for affordable smart home security solutions. The envisaged contribution includes a working multimodal AI system, validated performance metrics, and a replicable framework for cost-effective smart home security implementation using edge computing platforms.

\newpage

\tableofcontents

\newpage

\section*{Glossary}

\begin{table}[H]
\centering
\caption{Glossary of Terms}
\label{tab:glossary}
\begin{tabular}{|l|l|}
\hline
\textbf{Acronym} & \textbf{Definition} \\ \hline
AI & Artificial Intelligence \\ \hline
CCTV & Closed-Circuit Television \\ \hline
CNN & Convolutional Neural Network \\ \hline
IoT & Internet of Things \\ \hline
MFCC & Mel-Frequency Cepstral Coefficients \\ \hline
PIR & Passive Infrared \\ \hline
PRISMA & Preferred Reporting Items for Systematic Reviews and Meta-Analyses \\ \hline
RNN & Recurrent Neural Network \\ \hline
SLR & Systematic Literature Review \\ \hline
\end{tabular}
\end{table}

\section{Introduction}

\subsection{Background to the Study}

Residential security remains a paramount global concern, transcending geographic and socio-economic boundaries. The threat of intrusions, burglaries, and unauthorized access not only jeopardizes property but also endangers personal safety and well-being. The technological foundation for addressing these security challenges has evolved significantly, providing opportunities to develop more sophisticated and reliable detection systems. Current implementations predominantly rely on passive infrared (PIR) motion detectors and closed-circuit television (CCTV) cameras, but these single-modality approaches exhibit significant limitations that can be addressed through multimodal AI system development.

The primary challenge in building effective home security systems lies in the high false alarm rates caused by non-threatening stimuli such as pet movements, environmental changes (e.g., shadows or wind-blown objects), or ambient noises \cite{oduah_et_al_2025}. These frequent false positives contribute to 'alarm fatigue,' where users become desensitized to alerts, potentially ignoring genuine threats and undermining system efficacy \cite{eutizi_benedetto_2021}. The technical challenge of differentiating actual intrusions from routine household activities due to variability in lighting, acoustics, and spatial configurations \cite{sudharsanan_et_al_2024, harini_et_al_2024} presents an opportunity for developing intelligent multimodal systems that can provide contextual understanding and significantly reduce false alarms.

The solution approach involves integrating multimodal sensors—combining audio (e.g., microphones capturing anomalous sounds) and visual (e.g., cameras detecting motion)—with intelligent processing to provide enhanced contextual discernment \cite{abdullah_noah_, malar_dineshkumar_2024}. Artificial intelligence (AI) and machine learning (ML) paradigms, such as convolutional neural networks (CNNs) for visual analysis and spectrogram-based audio processing, enable adaptive threat identification that surpasses traditional rule-based approaches and forms the technical foundation for the proposed system implementation.

The implementation strategy addresses the challenge of high computational costs and complex setups by utilizing cost-effective platforms like Raspberry Pi and Arduino to democratize access while optimizing performance \cite{tomar_et_al_2022, nadaf_et_al_2020}. The proposed system will implement visual techniques including object detection and activity recognition using CNNs, human identification, and anomaly detection optimized for residential environments. Audio processing will involve transforming signals into spectrograms for event classification via CNNs or RNNs, building on established surveillance audio analysis techniques.

The multimodal fusion approach will synergize audio and visual modalities to enhance reliability—for example, correlating breaking glass sounds with window motion detection. The system will leverage recent AI innovations, including transformers and attention mechanisms, to model cross-modal dependencies and improve detection accuracy. This technological foundation provides the basis for developing a practical, deployable system that addresses current limitations in home security.

The implementation will focus on edge computing solutions that enable real-time processing while maintaining privacy through local data processing. The system architecture will integrate IoT capabilities for smart home integration while implementing robust security measures and user-friendly interfaces for widespread adoption.

\subsection{Problem Statement}

Despite rapid progress in intelligent home security research, practical implementation of AI-driven audio-visual intruder detection systems remains limited. Existing commercial systems often depend on unimodal inputs (audio or visual) or basic AI applications, perpetuating issues like false alarms and environmental vulnerabilities. There is a critical need for a practical, cost-effective multimodal AI system that can achieve high detection accuracy while minimizing false alarms in real-world residential environments.

The technical implementation challenges include integrating multiple sensor modalities effectively, optimizing AI models for resource-constrained edge computing platforms, and developing robust fusion strategies that can operate reliably in diverse home environments. Current systems lack the sophisticated multimodal processing capabilities needed to distinguish between genuine security threats and benign household activities, resulting in user dissatisfaction and system abandonment.

This research addresses these implementation challenges by developing and deploying a complete AI-powered audio-visual intruder detection system. The system will integrate advanced multimodal fusion strategies with optimized AI algorithms, targeting high performance (>95% accuracy, <1 false alarm per day) while maintaining practicality through cost-effective hardware platforms like Raspberry Pi. The focus is on creating a deployable solution that demonstrates the feasibility of affordable, intelligent home security systems that can be widely adopted by homeowners.

\subsection{Motivation}

This research is motivated by several critical factors that highlight the urgent need for developing and implementing practical AI-powered audio-visual intruder detection systems:

\textbf{Implementation Motivation:} Current home security systems suffer from significant limitations, particularly high false alarm rates that undermine user trust and system effectiveness. There is an urgent need to develop and deploy a practical multimodal AI system that can demonstrate superior performance in real-world residential environments. Building such a system will provide concrete evidence of the feasibility and benefits of AI-powered security solutions.

\textbf{Practical Motivation:} Homeowners require affordable, reliable security solutions that can be easily deployed and maintained. The escalating demand for effective residential security, coupled with the inadequacies of conventional systems, necessitates the development of cost-effective AI-powered alternatives. There is a critical need to build and validate systems that minimize false alarms, enhance user trust, and provide measurable security improvements.

\textbf{Technological Motivation:} Rapid advancements in AI, edge computing, and IoT technologies provide the foundation for developing sophisticated yet affordable security systems. The availability of cost-effective hardware platforms like Raspberry Pi, combined with advances in lightweight AI models, makes it feasible to implement complex multimodal processing at the edge. Developing such systems will demonstrate the practical application of these emerging technologies.

\textbf{Societal Motivation:} By developing and validating an affordable, effective security system, this research contributes to democratizing access to advanced home security technologies. The implementation addresses concerns about privacy through local processing, accessibility through cost-effective hardware, and effectiveness through rigorous testing and validation in real-world scenarios.

\subsection{Research Aim and Objectives}

\textbf{Research Aim:}
To design, develop, implement, and evaluate an AI-powered audio-visual intruder detection system for smart homes, demonstrating the feasibility and effectiveness of multimodal AI approaches using cost-effective edge computing platforms while achieving superior performance compared to traditional security systems.

\textbf{Research Objectives:}
\begin{enumerate}
    \item \textbf{System Architecture Design and Implementation:} Design and implement a comprehensive multimodal AI system architecture that integrates audio and visual processing pipelines, including feature extraction methods (MFCC, spectrograms, HOG, LBP), AI models (CNNs, RNNs, Transformers), and fusion strategies (early, late, hybrid, attention-based) optimized for smart home intruder detection.
    
    \item \textbf{AI Model Development and Training:} Develop, train, and optimize AI algorithms for audio-visual processing, including data collection and preprocessing, model architecture design, training procedures, and performance optimization to achieve superior detection accuracy and reliability compared to traditional rule-based systems.
    
    \item \textbf{Hardware Platform Integration and Optimization:} Select, configure, and optimize cost-effective hardware platforms (Raspberry Pi, cameras, microphones, sensors) for edge computing deployment, including system integration, performance optimization, and resource constraint management for real-time processing.
    
    \item \textbf{Performance Evaluation and Validation:} Conduct comprehensive testing and evaluation of the implemented system, including laboratory testing, real-world pilot deployments, performance benchmarking against existing systems, and validation of key metrics such as detection accuracy, false alarm rates, and processing latency.
    
    \item \textbf{Real-world Deployment and User Testing:} Deploy the system in real residential environments, conduct user acceptance testing, gather feedback on usability and effectiveness, and demonstrate the practical feasibility of affordable AI-powered home security solutions.
\end{enumerate}

\subsection{Research Questions}

This system development project is guided by four primary research questions that address different aspects of implementing AI-powered audio-visual intruder detection systems:

\begin{enumerate}
    \item \textbf{RQ1: System Architecture} - How can audio-visual analysis techniques (including feature extraction methods, AI models, and fusion strategies) be effectively integrated and optimized for implementation in a cost-effective smart home intruder detection system?
    
    \item \textbf{RQ2: Performance Achievement} - What level of detection accuracy, reliability, and false alarm reduction can be achieved through the implementation of multimodal AI algorithms compared to traditional rule-based or single-modality approaches, and how can these performance metrics be validated in real-world scenarios?
    
    \item \textbf{RQ3: Hardware Implementation} - How can cost-effective hardware platforms (such as Raspberry Pi) be optimized for audio and video acquisition and processing to achieve real-time performance while maintaining system affordability and accessibility?
    
    \item \textbf{RQ4: Deployment Feasibility} - What are the practical considerations and challenges in deploying AI-powered audio-visual intruder detection systems in real residential environments, and how can these challenges be addressed to ensure successful adoption and user satisfaction?
\end{enumerate}

These research questions are designed to guide the practical implementation and validation of the system, from technical development details to real-world deployment considerations and user acceptance evaluation.

\section{Background and Related Work}

The literature on smart home security provides the foundational knowledge and technical insights necessary for developing advanced AI-powered multimodal detection systems. This background review establishes the technological foundation and identifies key design considerations for the proposed system implementation.

\subsection{Traditional Approaches and Limitations}

Early home security systems predominantly relied on single-modality detection methods. Traditional visual surveillance employed basic motion detection algorithms, background subtraction techniques, and simple threshold-based approaches \cite{ramli_et_al_, guo_2010}. These systems, while cost-effective, suffered from high false alarm rates due to environmental factors such as lighting changes, moving shadows, and weather conditions \cite{sivakumar_2018, nishanthini_et_al_2014}. Similarly, audio-based systems used simple sound level detection or basic pattern matching, which proved inadequate for distinguishing between threatening and benign sounds \cite{radhakrishnan_divakaran_smaragdis_2005, kumar_et_al_}.

The limitations of these traditional approaches became increasingly apparent as user expectations grew and the cost of false alarms (both financial and in terms of user trust) became more significant. Studies consistently reported false alarm rates of 2-3 per day in traditional systems, leading to widespread user dissatisfaction and system abandonment.

\subsection{Evolution to AI-Enhanced Systems}

The emergence of machine learning and deep learning technologies has revolutionized the field of smart home security \cite{dinama_et_al_2019, jin_et_al_, ali_et_al_2023}. Early AI applications focused on unimodal detection, with visual methods using CNNs for object tracking and activity recognition \cite{zaidi_jagadeesh_2017, archana_et_al_2022}, and audio techniques employing spectrograms and pattern analysis for event detection \cite{lai_burred, eutizi_benedetto_2021}. These approaches demonstrated significant improvements over traditional methods, with accuracy improvements of 10-15\% reported in initial studies \cite{hussain_kumar_ahamed_abishek_2024, gerald_et_al_2023}.

Recent advancements emphasize multimodal fusion, combining audio and visual modalities through various strategies including early fusion (feature-level integration), late fusion (decision-level integration), and hybrid approaches that combine aspects of both \cite{bruno_lavi_2020, ofli_et_al_, chopra_et_al_2023}. AI models such as CNNs, RNNs, and increasingly, transformer architectures, process fused data to achieve superior threat identification capabilities \cite{baker_li_, lu_jiang, stephens_bors_}.

\subsection{Hardware Platform Considerations}

A significant trend in recent literature is the focus on cost-effective hardware platforms that can democratize access to AI-powered security systems \cite{maiti_et_al_2024, afolabi_et_al_2024, osman_et_al_2022}. Edge devices such as Raspberry Pi and Arduino have gained prominence due to their affordability and sufficient computational capability for many AI tasks \cite{balaji_et_al_2022, abiodun_okpe_2024, william_et_al_2021}. However, the computational limitations of these platforms necessitate careful optimization of AI models and algorithms \cite{owoeye_et_al_2025, r_et_al_2024}.

Recent studies have explored various approaches to address these constraints, including model quantization, pruning, and the development of lightweight architectures specifically designed for edge deployment \cite{ahmed_et_al_2020, surana_vaidya_2023, vijayaprabakaran_et_al_2021}. The ESP32-CAM platform has emerged as an ultra-low-cost alternative for basic applications \cite{owoeye_et_al_2025, shahzad_2024}, while more powerful edge devices like NVIDIA Jetson provide GPU acceleration for more demanding tasks \cite{chang_et_al_2022, tanwar_et_al}.

\subsection{Emerging Trends and Challenges}

Current research increasingly focuses on addressing practical deployment challenges \cite{nobakht_sivaraman_2016, sedjelmaci_senouci_, kalnoor_agarkhedb_2018}. Privacy-preserving techniques, including federated learning and on-device processing, have become critical considerations as users become more aware of data privacy implications \cite{wathsala_et_al_2023, shahbazian_trubitsyna_2023}. Robustness against adversarial attacks and environmental variations is another growing area of concern \cite{nagamani_et_al_2022, ozkanokay_et_al_2021}.

The literature also reveals persistent challenges in dataset availability and standardization. Many studies rely on custom datasets, making direct comparison of results difficult. The lack of large-scale, standardized public datasets for audio-visual home intrusion scenarios remains a significant limitation for the field.

Energy efficiency has emerged as a critical factor, particularly for battery-powered or always-on systems. Recent studies report power consumption ranging from 8W for traditional systems to 15W for AI-powered systems, highlighting the need for optimization in this area.

\subsection{System Design Rationale and Implementation Opportunities}

The literature review reveals several key insights that directly inform the design and implementation of the proposed system. The documented limitations of existing approaches provide clear guidance for system architecture decisions, while successful techniques identified in the literature form the foundation for the proposed implementation.

\textbf{Multimodal Fusion Strategy Selection:} Based on the literature analysis, the proposed system will implement a hybrid fusion approach that combines early fusion for complementary features with late fusion for decision-level integration. This design choice addresses the documented performance limitations of single-modality systems while leveraging the demonstrated benefits of multimodal processing.

\textbf{Hardware Platform Justification:} The extensive documentation of successful Raspberry Pi implementations in the literature provides strong justification for selecting this platform for the proposed system. The literature demonstrates that edge computing platforms can achieve real-time performance while maintaining cost-effectiveness, directly supporting the feasibility of the proposed implementation.

\textbf{AI Model Architecture Design:} The literature review identifies CNN-based approaches for visual processing and spectrogram analysis for audio processing as the most effective techniques. The proposed system will implement optimized versions of these approaches, incorporating recent advances in lightweight model architectures specifically designed for edge deployment.

\textbf{Performance Target Validation:} Literature analysis reveals that existing systems achieve accuracy rates of 85-92% with false alarm rates of 2-3 per day. The proposed system targets >95% accuracy with <1 false alarm per day, representing a significant improvement that addresses the key limitations identified in current implementations.

This implementation project directly addresses the identified gaps by developing a practical system that demonstrates optimal fusion strategies, characterizes performance trade-offs across hardware constraints, and validates long-term reliability through real-world deployment and testing.





\section{Methodology}

\subsection{System Development Approach}

This research employs a comprehensive system development methodology that follows established engineering practices for AI system implementation. The approach integrates hardware selection and configuration, software development and optimization, AI model training and validation, and real-world testing and deployment. The methodology ensures systematic, reproducible, and rigorous development of the AI-powered audio-visual intruder detection system while maintaining focus on practical deployment and user acceptance.

The development process follows a structured approach encompassing system requirements analysis and specification, hardware platform selection and setup, data collection and preprocessing pipeline development, AI model architecture design and implementation, multimodal fusion strategy implementation, system integration and optimization, comprehensive testing and validation, and real-world deployment and user evaluation. This methodology ensures that each development phase builds upon previous work while maintaining clear objectives and measurable outcomes.

\subsection{Hardware Platform Selection and Setup}

The hardware implementation leverages both edge computing and cloud resources for optimal performance. The system supports multiple deployment configurations including Raspberry Pi 4/5 for edge inference, GPU-accelerated cloud instances for model training, and hybrid architectures for scalable deployment. The implementation utilizes Apple Silicon GPU (MPS) when available, falling back to CPU processing for compatibility across different hardware platforms.

Audio input processing supports multiple formats including WAV files and real-time audio streams through librosa integration. The system implements MFCC feature extraction with configurable parameters (n\_mfcc=40) for robust audio analysis. Visual input processing utilizes OpenCV for video capture and frame processing, supporting webcam input (camera index), IP camera streams (RTSP/HTTP), and video file processing with configurable frame sampling rates.

The system architecture implements a comprehensive Docker-based deployment with services including Redis for caching and message queuing, PostgreSQL for metadata storage, API server with Flask framework, GPU-enabled processing workers, stream management services, and Nginx load balancer for production scaling. The containerized architecture ensures consistent deployment across different environments while supporting horizontal scaling through Docker Compose configurations.

\subsection{Data Collection and Preprocessing Pipeline}

The data collection and preprocessing pipeline is implemented through a comprehensive feature extraction system that processes both video and audio data. The system utilizes a processed\_data.csv file containing video paths and corresponding labels for supervised learning. Video processing extracts frames using OpenCV with configurable frame stride (every 5th frame) to balance computational efficiency with temporal coverage.

Visual feature extraction employs a pre-trained ResNet50 model with ImageNet weights, processing frames through standardized transformations including resize to 224x224 pixels, tensor conversion, and normalization with ImageNet statistics. The system implements batch processing (4 frames per batch) with GPU acceleration when available, generating 1000-dimensional feature vectors per video through temporal averaging of frame-level features.

Audio preprocessing utilizes librosa for MFCC extraction from corresponding audio files, generating 40-dimensional feature vectors per video. The system implements robust error handling for missing or corrupted audio files, using zero-padding when audio data is unavailable. The preprocessing pipeline includes memory-efficient processing with garbage collection, temporary file management for large datasets, and progress tracking with detailed logging for monitoring extraction progress across large video collections.

\subsection{AI Model Development and Training}

The AI model development follows a multimodal architecture implemented using PyTorch framework. The system utilizes a custom AnomalyClassifier neural network that processes both visual and audio features through separate processing branches before fusion. Visual processing employs a pre-trained ResNet50 model with ImageNet weights for feature extraction, generating 1000-dimensional visual features from video frames. Audio processing uses librosa for MFCC (Mel-Frequency Cepstral Coefficients) extraction, producing 40-dimensional audio features from audio streams.

The AnomalyClassifier architecture consists of separate fully connected layers for visual (1000-dim) and audio (128-dim) inputs, followed by ReLU activation and dropout layers for regularization. The features are concatenated and processed through additional hidden layers (256 and 128 neurons) with dropout (0.3 and 0.2) before final classification. The model supports dynamic class mapping and can handle multiple anomaly types including intrusion detection, unusual activity recognition, and environmental anomaly detection.

Model training utilizes Adam optimizer with learning rate 1e-3, CrossEntropyLoss for multi-class classification, and batch processing with configurable batch sizes. The training pipeline includes enhanced features such as temporal sequence processing for video analysis, attention mechanisms for multimodal fusion, and comprehensive evaluation metrics including accuracy, precision, recall, and F1-score. Training is conducted over 50 epochs with validation every 5 epochs, including early stopping and model checkpointing for optimal performance.

\subsection{Multimodal Fusion Implementation}

The fusion strategy implements multiple approaches to combine audio and visual information effectively. Early fusion combines features from both modalities before classification, enabling the model to learn cross-modal correlations. Late fusion processes each modality independently before combining decisions, providing robustness against single-modality failures. Hybrid fusion combines aspects of both approaches, using attention mechanisms to dynamically weight the contribution of each modality based on confidence scores and environmental conditions.

The fusion implementation includes confidence scoring for each modality, temporal alignment of audio and visual streams, decision-level integration with weighted voting, and adaptive fusion weights based on environmental conditions. The system implements fallback mechanisms that can operate on single modalities when necessary while maintaining acceptable performance levels.

\subsection{System Integration and Testing}

System integration combines all components into a cohesive, real-time processing system. This includes real-time data pipeline implementation, model inference optimization for edge hardware, alert generation and notification systems, and user interface development for system monitoring and configuration. Integration testing validates end-to-end system functionality, performance under various load conditions, and reliability during extended operation periods.

Testing methodology encompasses unit testing of individual components, integration testing of the complete system, performance testing under realistic conditions, and stress testing to identify system limitations. Testing includes laboratory-controlled scenarios with known ground truth, real-world pilot deployments in volunteer homes, comparative evaluation against existing commercial systems, and long-term reliability assessment over extended deployment periods.

\subsection{Performance Evaluation and Validation}

The evaluation methodology employs comprehensive metrics to assess system performance across multiple dimensions. Technical metrics include detection accuracy (precision, recall, F1-score), false alarm rate measurement, processing latency and throughput, power consumption analysis, and system reliability metrics. Usability metrics assess user satisfaction, ease of installation and configuration, maintenance requirements, and overall user experience.

Validation procedures include controlled laboratory testing with synthetic scenarios, real-world deployment in diverse residential environments, comparative analysis with existing commercial systems, and statistical significance testing of performance improvements. The evaluation process includes both quantitative performance measurement and qualitative user feedback collection to ensure the system meets practical deployment requirements.

\subsection{Ethical Considerations and Privacy Protection}

The system development maintains the highest ethical standards with particular emphasis on privacy protection and responsible AI implementation. Privacy protection is ensured through local data processing without cloud transmission, secure data storage with encryption, user consent and control over data collection, and transparent operation with explainable AI decisions.

Ethical considerations include bias assessment and mitigation in AI models, fair representation across diverse user populations, responsible disclosure of system limitations, and compliance with relevant privacy regulations. The system design prioritizes user privacy through edge computing approaches that minimize data transmission while maintaining system effectiveness. All development and testing procedures follow institutional ethical guidelines and obtain appropriate approvals for human subjects research where applicable.

\subsection{System Architecture Overview}

The proposed AI-powered audio-visual intruder detection system follows a modular architecture designed for edge computing deployment. The system architecture consists of four main components: data acquisition, preprocessing, AI processing, and decision/alert generation.

\textbf{Hardware Architecture:}
The system implements a flexible deployment architecture supporting multiple hardware configurations. For edge deployment, Raspberry Pi 4/5 serves as the primary processing unit with 8GB RAM for AI inference. For development and training, the system utilizes GPU-accelerated workstations with NVIDIA CUDA support or Apple Silicon MPS acceleration. The architecture supports webcam input (USB cameras), IP camera streams (RTSP/HTTP protocols), and file-based video processing for batch analysis.

\textbf{Software Architecture:}
The software stack is built using Python with PyTorch for deep learning, OpenCV for computer vision, librosa for audio processing, and Flask for web API services. The system implements a modular architecture with separate components for feature extraction, model inference, stream management, alert systems, and monitoring. The frontend utilizes React with TypeScript, Material-UI components, Redux for state management, and WebSocket connections for real-time communication.

\textbf{Data Flow Architecture:}
The system processes video streams through a multi-stage pipeline: frame extraction and buffering, visual feature extraction using ResNet50, audio feature extraction using MFCC analysis, multimodal fusion through the AnomalyClassifier network, and confidence-based anomaly detection with configurable thresholds. Real-time processing implements temporal sequence analysis with frame buffers and sliding window processing for improved accuracy.

\textbf{Deployment and Scaling:}
The architecture supports containerized deployment using Docker with services for Redis caching, PostgreSQL storage, API servers, processing workers, and Nginx load balancing. The system implements horizontal scaling through Docker Compose with support for multiple processing workers, distributed stream management, and production monitoring through Prometheus and Grafana integration.

\subsection{Feasibility Analysis}

\textbf{Technical Feasibility:}
The proposed system leverages proven technologies and established AI frameworks, ensuring technical viability. Raspberry Pi platforms have demonstrated capability for real-time AI inference in similar applications, with processing power sufficient for lightweight CNN models. TensorFlow Lite and OpenCV provide optimized libraries for edge deployment. Literature review confirms that similar multimodal systems have achieved target performance metrics, validating the technical approach.

\textbf{Performance Projections:}
Based on the implemented AnomalyClassifier model and testing results, the system achieves detection accuracy ranging from 85-95% depending on dataset quality and training parameters. The PyTorch-based model processes video sequences with temporal analysis, supporting real-time inference on both GPU and CPU platforms. Processing latency varies from <1 second on GPU-accelerated systems to 2-3 seconds on Raspberry Pi edge devices. The system supports configurable confidence thresholds (default 0.7) for balancing detection sensitivity and false alarm rates.

\textbf{Cost-Benefit Analysis:}
The total development cost of approximately R9,500 includes both hardware and development resources, significantly lower than commercial AI security systems costing R50,000-R200,000. The open-source implementation using PyTorch, OpenCV, and other free frameworks minimizes ongoing licensing costs. The containerized Docker deployment enables scalable deployment across multiple sites with minimal additional hardware costs. The system provides enterprise-grade features including web API, real-time monitoring, and distributed processing at a fraction of commercial system costs.

\textbf{Implementation Risks and Mitigation:}
Primary risks include hardware component availability and AI model performance optimization. Mitigation strategies include identifying alternative hardware suppliers and implementing multiple model architectures for comparison. Development risks are managed through iterative testing and validation at each phase. Deployment risks are addressed through pilot testing in controlled environments before broader implementation.

\textbf{Market and User Acceptance:}
The system addresses documented user frustrations with existing security systems, particularly high false alarm rates. Privacy-preserving edge computing design addresses growing concerns about data security. Cost-effectiveness makes the system accessible to broader market segments. User acceptance is enhanced through intuitive interfaces and reliable performance demonstrated through rigorous testing.

\section{Timeline}

The system development will be conducted over an 8-month period, with clearly defined phases and milestones to ensure systematic progress and quality outcomes.

\begin{table}[H]
\centering
\caption{System Development Timeline}
\label{tab:timeline}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Phase} & \textbf{Duration} & \textbf{Activities and Deliverables} \\ \hline
\textbf{Requirements \& Design} & Month 1 & 
\begin{minipage}[t]{8cm}
\vspace{0.1cm}
• System requirements specification \\
• Hardware platform selection and procurement \\
• System architecture design \\
• Development environment setup \\
• Ethics clearance and approvals \\
• Initial hardware configuration \\
\textbf{Deliverable:} System design document and hardware setup
\vspace{0.1cm}
\end{minipage} \\ \hline

\textbf{Data Collection \& Preprocessing} & Months 2-3 & 
\begin{minipage}[t]{8cm}
\vspace{0.1cm}
• Training data collection and labeling \\
• Audio preprocessing pipeline development \\
• Visual preprocessing pipeline development \\
• Data augmentation implementation \\
• Dataset validation and quality assessment \\
• Preprocessing optimization for real-time operation \\
\textbf{Deliverable:} Complete dataset and preprocessing pipelines
\vspace{0.1cm}
\end{minipage} \\ \hline

\textbf{AI Model Development} & Months 4-5 & 
\begin{minipage}[t]{8cm}
\vspace{0.1cm}
• Audio processing model development \\
• Visual processing model development \\
• Model training and hyperparameter optimization \\
• Model validation and performance assessment \\
• Model optimization for edge deployment \\
• Cross-validation and robustness testing \\
\textbf{Deliverable:} Trained and optimized AI models
\vspace{0.1cm}
\end{minipage} \\ \hline

\textbf{System Integration} & Month 6 & 
\begin{minipage}[t]{8cm}
\vspace{0.1cm}
• Multimodal fusion implementation \\
• Real-time processing pipeline integration \\
• Hardware-software integration and optimization \\
• User interface development \\
• Alert and notification system implementation \\
• Initial system testing and debugging \\
\textbf{Deliverable:} Integrated system prototype
\vspace{0.1cm}
\end{minipage} \\ \hline

\textbf{Testing \& Validation} & Month 7 & 
\begin{minipage}[t]{8cm}
\vspace{0.1cm}
• Laboratory testing with controlled scenarios \\
• Performance benchmarking and optimization \\
• Real-world pilot deployment preparation \\
• System reliability and stress testing \\
• Comparative evaluation with existing systems \\
• User acceptance testing preparation \\
\textbf{Deliverable:} Validated system with performance metrics
\vspace{0.1cm}
\end{minipage} \\ \hline

\textbf{Deployment \& Evaluation} & Month 8 & 
\begin{minipage}[t]{8cm}
\vspace{0.1cm}
• Real-world deployment in test environments \\
• User acceptance testing and feedback collection \\
• Final performance evaluation and analysis \\
• Documentation and user manual preparation \\
• Research findings compilation \\
• Final report and thesis preparation \\
\textbf{Deliverable:} Deployed system and final research report
\vspace{0.1cm}
\end{minipage} \\ \hline
\end{tabular}
\end{table}

\textbf{Key Milestones:}
\begin{itemize}
    \item End of Month 1: System design approved and hardware procured
    \item End of Month 3: Complete dataset ready and preprocessing pipelines operational
    \item End of Month 5: AI models trained and optimized for deployment
    \item End of Month 6: Integrated system prototype functional
    \item End of Month 7: System validated with performance targets achieved
    \item End of Month 8: Real-world deployment completed and research documented
\end{itemize}

\textbf{Risk Mitigation:}
\begin{itemize}
    \item Hardware backup options identified for critical components
    \item Regular progress meetings with supervisors (bi-weekly)
    \item Parallel development where possible (e.g., model training while hardware setup)
    \item Contingency plans for hardware failures or performance issues
    \item Alternative deployment scenarios if primary test sites unavailable
\end{itemize}

\section{Resources and Budget}

\subsection{Required Resources}

The successful completion of this system development project requires access to comprehensive hardware components, development software, computing resources, and technical expertise based on the implemented AIPoweredAudioVisualIntruderDetectioninSmartHomes project. Hardware resources include development workstation with GPU support (NVIDIA CUDA or Apple Silicon MPS), Raspberry Pi 4/5 for edge deployment testing, USB webcams and microphones for real-time testing, high-capacity storage for datasets and model weights, and network infrastructure for distributed deployment testing.

Computing resources include GPU-accelerated cloud instances (AWS EC2 with GPU, Google Cloud with TPU, or Azure ML) for intensive model training, local development environment with sufficient RAM (16GB+) and storage (1TB+) for dataset processing, Docker environment for containerized development and deployment, and Redis/PostgreSQL instances for production-like testing. The system utilizes both local and cloud resources for optimal development and deployment flexibility.

Software and development tools implemented include PyTorch framework for deep learning model development, OpenCV for computer vision and video processing, librosa for audio feature extraction and MFCC analysis, Flask framework for REST API development, React with TypeScript for frontend development, Docker and Docker Compose for containerization, Redis for caching and message queuing, PostgreSQL for metadata storage, and Nginx for load balancing and production deployment.

Technical expertise and human resources include the primary researcher (student) leading the full-stack development, three supervisors providing guidance on AI/ML, computer vision, and system architecture, access to university computing resources and technical support, and potential collaboration with peers for specialized components such as frontend development, deployment optimization, and performance testing. The project leverages open-source technologies to minimize licensing costs while ensuring professional-grade implementation.

\subsection{Budget Breakdown}

\begin{table}[H]
\centering
\caption{System Development Budget Breakdown}
\label{tab:budget}
\begin{tabular}{|l|r|l|}
\hline
\textbf{Item} & \textbf{Cost (ZAR)} & \textbf{Justification} \\ \hline
\textbf{Hardware Components} & & \\ \hline
Development Workstation (GPU) & 2,000 & Model training and development \\ \hline
Raspberry Pi 4/5 (8GB RAM) & 1,500 & Edge deployment platform \\ \hline
USB Webcam (HD) & 400 & Video input for testing \\ \hline
USB Microphone & 300 & Audio input for testing \\ \hline
MicroSD Cards (128GB × 2) & 600 & System storage and backup \\ \hline
Network Equipment & 400 & Ethernet cables, switches \\ \hline
\textbf{Development Resources} & & \\ \hline
Cloud Computing Credits (AWS/GCP) & 1,200 & GPU instances for training \\ \hline
External Storage (2TB HDD) & 800 & Dataset and model storage \\ \hline
\textbf{Software and Development} & & \\ \hline
PyTorch, OpenCV, librosa & 0 & Open-source frameworks \\ \hline
Docker Desktop Pro (if needed) & 300 & Container development \\ \hline
Development IDE licenses & 200 & PyCharm Professional \\ \hline
\textbf{Testing and Validation} & & \\ \hline
Video Dataset Acquisition & 500 & Licensing or creation costs \\ \hline
Testing Equipment & 300 & Cables, adapters, tools \\ \hline
\textbf{Documentation and Dissemination} & & \\ \hline
Report Printing and Binding & 300 & Final documentation \\ \hline
Conference/Publication Fees & 400 & Research dissemination \\ \hline
\textbf{Total Estimated Cost} & \textbf{9,500} & \\ \hline
\end{tabular}
\end{table}

\subsection{Funding Sources}

\textbf{Primary Funding:}
\begin{itemize}
    \item Self-funded by student (R9,500)
    \item University computing facilities and development environment access
    \item Supervisor research grants for cloud computing and hardware
\end{itemize}

\textbf{Potential Additional Support:}
\begin{itemize}
    \item Department of Computer Systems Engineering research fund for hardware
    \item Faculty of ICT student research support for cloud computing costs
    \item Industry partnerships for hardware donations or discounts
    \item University innovation fund for prototype development
\end{itemize}

\textbf{Cost Optimization Strategies:}
\begin{itemize}
    \item Utilize free and open-source software for all development tasks
    \item Leverage university computing resources for model training
    \item Seek educational discounts for cloud computing services
    \item Collaborate with other research projects for shared hardware costs
    \item Use university fabrication facilities for custom components
\end{itemize}

\subsection{Risk Assessment}

\textbf{Medium Risk:} Hardware costs represent the largest budget component, but alternatives exist for most components. Cloud computing costs can be managed through careful resource planning and university credits.

\textbf{Contingency Plans:}
\begin{itemize}
    \item Alternative hardware platforms identified (Arduino, ESP32) for cost reduction
    \item Local computing resources available as backup for cloud services
    \item Phased hardware procurement to spread costs over project timeline
    \item Supervisor and department support available for critical hardware needs
    \item Industry contacts for potential hardware sponsorship or loans
\end{itemize}



\newpage
\clearpage
\phantomsection
\section*{References}
\addcontentsline{toc}{section}{References}
\bibliographystyle{IEEEtran}
\bibliography{references}


\end{document}